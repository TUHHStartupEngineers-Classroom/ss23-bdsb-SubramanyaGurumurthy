[
  {
    "objectID": "content/01_journal/01_tidyverse.html",
    "href": "content/01_journal/01_tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Tidyverse is a collection of R packages designed to make data manipulation, visualization, and analysis easier and more consistent. It provides a set of tools and functions that follow a consistent grammar and syntax, making it easier to work with data in a tidy and organized manner."
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#solution",
    "href": "content/01_journal/01_tidyverse.html#solution",
    "title": "Tidyverse",
    "section": "\n1.1 Solution:",
    "text": "1.1 Solution:\n\nLoad necessary libraries: This code block loads the required libraries, dplyr and ggplot2, which are used for data manipulation and plotting, respectively.\nRead in the data file: The code reads an Excel file named “bike_orderlines_new.xlsx” and assigns it to the variable bike_orderlines_wrangled_tbl_new. The data is stored as a tibble, a modern version of a data frame. Please take care of path to read the file.\nSplit the location data: The separate() function is used to split the “location” column of the bike_orderlines_wrangled_tbl_new tibble into two separate columns, “city” and “state”. The separator used is “,” (comma followed by a space). The original “location” column is retained in the tibble.\nGroup the data and calculate total revenue: The group_by() function groups the data in bike_orderlines_wrangled_tbl_new by “state”. Then, the summarize() function calculates the total revenue by summing the “total_price” column. The results are stored in the sales_by_loc_tbl_new tibble.\nCreate a bar plot: The ggplot() function is used to initialize a new ggplot object, with sales_by_loc_tbl_new as the data. The aesthetic mappings are set with aes(x = state, y = total_revenue). The geom_bar() function is used to create a bar plot with “state” on the x-axis and “total_revenue” on the y-axis. Additional formatting and labeling options are set using labs() and theme() functions.\nGroup the data by state and year and calculate total revenue: Similar to step 4, this code block groups the data by both “state” and “model_year” columns and calculates the total revenue for each group. The results are stored in the sales_by_loc_year_tbl_new tibble.\nCreate a bar plot with facets: This code block creates a bar plot of the total revenue by “model_year” for each “state”. The facet_wrap() function is used to create a grid of plots, with each plot representing a different state. The scales = “free_x” argument allows each facet to have an independent x-axis scale. The ncol = 4 argument sets the number of columns in the grid to 4.\n\nThe Complete solution to challenge:\n\n##############################################################################################\n#Assignment task\n##############################################################################################\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(readxl)\nlibrary(tidyverse)\n\n# Read in the data file\nbike_orderlines_wrangled_tbl_new <- read_excel(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/ds_data/01_bike_sales/02_wrangled_data/bike_orderlines_new.xlsx\")\n\n# Split the location data into separate columns for state and city\nbike_orderlines_wrangled_tbl_new <- bike_orderlines_wrangled_tbl_new %>%\nseparate(location, into = c(\"city\", \"state\"), sep = \", \", remove = FALSE)\n\n# Group the data by state and calculate the total revenue\nsales_by_loc_tbl_new <- bike_orderlines_wrangled_tbl_new %>%\ngroup_by(state) %>%\nsummarize(total_revenue = sum(total_price))\n\n# Create a bar plot of the total revenue by state#\nggplot(sales_by_loc_tbl_new, aes(x = state, y = total_revenue)) +\ngeom_bar(stat = \"identity\", fill = \"steelblue\") +\nlabs(title = \"Total Revenue by State\", x = \"State\", y = \"Total Revenue\") +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n# Group the data by state and year and calculate the total revenue\nsales_by_loc_year_tbl_new <- bike_orderlines_wrangled_tbl_new %>%\ngroup_by(state, model_year) %>%\nsummarize(total_revenue = sum(total_price))\n\n#> `summarise()` has grouped output by 'state'. You can override using the\n#> `.groups` argument.\n\n# Create a bar plot of the total revenue by state and year\nggplot(sales_by_loc_year_tbl_new, aes(x = model_year, y = total_revenue)) +\ngeom_bar(stat = \"identity\", fill = \"steelblue\") +\nlabs(title = \"Total Revenue by State and Year\", x = \"Year\", y = \"Total Revenue\") +\ntheme(axis.text.x = element_text(angle = 45, hjust = 1)) +\nfacet_wrap(~state, scales = \"free_x\", ncol = 4)"
  },
  {
    "objectID": "content/01_journal/01_tidyverse.html#result-plots",
    "href": "content/01_journal/01_tidyverse.html#result-plots",
    "title": "Tidyverse",
    "section": "\n1.2 Result Plots:",
    "text": "1.2 Result Plots:\nThese images are obtained while running the program on local computer.\n\n\nToal Revenue by State\n\n\n\n\nToal Revenue by State and Year"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html",
    "href": "content/01_journal/02_data_acquisition.html",
    "title": "Data Acquisition",
    "section": "",
    "text": "Data acquisition in R refers to the process of obtaining data from various sources and loading it into R for analysis and manipulation. R provides several functions and packages that facilitate data acquisition from different types of sources, such as files, databases, web APIs, and more. Here is an introduction to some common methods of data acquisition in R:\nReading from Files:\n\nread.csv(): Reads data from a CSV (Comma-Separated Values) file.\nread.table(): Reads data from a delimited text file.\nread_excel(): Reads data from an Excel file.\nreadr package: Provides efficient functions for reading various file formats.\n\nConnecting to Databases:\n\nDBI package: Provides a consistent interface for connecting to and querying databases.\ndbConnect(): Establishes a connection to a database.\ndbGetQuery(): Executes a SQL query and retrieves data from a database.\n\nWeb Scraping:\n\nrvest package: Allows scraping data from websites using CSS selectors or XPath expressions.\nhttr package: Provides functions for making HTTP requests and retrieving web content.\n\nWeb APIs:\n\nhttr package: Supports interacting with web APIs using HTTP requests (GET, POST, etc.).\njsonlite package: Enables parsing and manipulating data in JSON format.\n\nOther Sources:\n\nreadr and haven packages: Support reading data from SAS, SPSS, and Stata files.\nRODBC package: Connects to ODBC-compliant databases.\n\nThese are just a few examples of data acquisition methods in R. Depending on your specific needs, there may be other packages and functions available to acquire data from specific sources or formats. The choice of method depends on the type of data, its source, and the desired data format for further analysis in R."
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#solution-explaination",
    "href": "content/01_journal/02_data_acquisition.html#solution-explaination",
    "title": "Data Acquisition",
    "section": "\n2.1 Solution Explaination:",
    "text": "2.1 Solution Explaination:\n\nThe code loads the necessary packages: httr for making HTTP requests and jsonlite for parsing JSON data.\nThe desired location, “Hamburg,” is set.\nThe API request URL is constructed with the latitude, longitude, and the specific weather parameters to retrieve (temperature, relative humidity, precipitation probability, and rain).\nThe GET() function from httr sends the API request and retrieves the response.\nThe code checks if the request was successful by verifying the status code (200).\nIf the request was successful, the JSON response is parsed using fromJSON() from jsonlite. The flatten = TRUE argument ensures a flat structure for easier data extraction.\nThe relevant weather data (temperature, time, relative humidity, rain, and precipitation probability) is extracted from the parsed JSON.\nThe code then prints the location and the units of measurement for each weather parameter.\nThe extracted data is stored in a data frame (df).\nFinally, the data frame is printed to display the weather data.\n\nThe complete code:\n\n##############################################################################################\n#1st task\n##############################################################################################\n\nlibrary(httr)\nlibrary(jsonlite)\n\n# Set the desired location\nlocation <- \"Hamburg\"\n\n#the API request URL\nurl <- \"https://api.open-meteo.com/v1/forecast?latitude=48.75&longitude=9.10&hourly=temperature_2m,relativehumidity_2m,precipitation_probability,rain\"\n\n# Send the API request and retrieve the response\nresponse <- GET(url)\n\n\n# Check if the request was successful (status code 200)\nif (status_code(response) == 200) {\n  # Parse the JSON response\n  data <- fromJSON(content(response, \"text\"), flatten = TRUE)\n  \n  # Extract the relevant data\n  temperature <- data$hourly$temperature_2m\n  time <- data$hourly$time\n  relativehumidity <- data$hourly$relativehumidity_2m\n  rain <- data$hourly$rain\n  precip_probability <- data$hourly$precipitation_probability\n  \n  # Print the weather data\n  cat(\"Weather in\", location, \"\\n\")\n  cat(\"Units:\\n\")\n  cat(\"Temperature:\",\"°C\\n\")\n  cat(\"Relative Humidity:\",\"%\\n\")\n  cat(\"Precipitation:\", \"mm\\n\")\n  cat(\"Rain:\",\"mm\\n\")\n  \n  df <- data.frame(\"Date_T_Time\" = time, \"Temperature\" = temperature, \"Relative_Humidity\" = relativehumidity,\n                   \"Rain\" = rain, \"Precipitation_Probability\" = precip_probability)\n  \n  print(head(df, 25))   \n  plot(df)\n  \n} else {\n  cat(\"Error:\", status_code(response), \"\\n\")\n}\n\n#> Weather in Hamburg \n#> Units:\n#> Temperature: °C\n#> Relative Humidity: %\n#> Precipitation: mm\n#> Rain: mm\n#>         Date_T_Time Temperature Relative_Humidity Rain\n#> 1  2023-05-20T00:00        10.3                79    0\n#> 2  2023-05-20T01:00        10.5                82    0\n#> 3  2023-05-20T02:00         9.8                87    0\n#> 4  2023-05-20T03:00         9.7                87    0\n#> 5  2023-05-20T04:00         9.6                89    0\n#> 6  2023-05-20T05:00        10.3                87    0\n#> 7  2023-05-20T06:00        11.2                85    0\n#> 8  2023-05-20T07:00        12.8                80    0\n#> 9  2023-05-20T08:00        14.3                72    0\n#> 10 2023-05-20T09:00        15.4                68    0\n#> 11 2023-05-20T10:00        16.6                64    0\n#> 12 2023-05-20T11:00        18.5                56    0\n#> 13 2023-05-20T12:00        19.7                50    0\n#> 14 2023-05-20T13:00        20.6                47    0\n#> 15 2023-05-20T14:00        20.0                50    0\n#> 16 2023-05-20T15:00        21.4                46    0\n#> 17 2023-05-20T16:00        21.1                49    0\n#> 18 2023-05-20T17:00        20.6                56    0\n#> 19 2023-05-20T18:00        19.8                65    0\n#> 20 2023-05-20T19:00        17.3                71    0\n#> 21 2023-05-20T20:00        16.2                74    0\n#> 22 2023-05-20T21:00        15.9                75    0\n#> 23 2023-05-20T22:00        15.7                74    0\n#> 24 2023-05-20T23:00        15.0                77    0\n#> 25 2023-05-21T00:00        14.6                79    0\n#>    Precipitation_Probability\n#> 1                          0\n#> 2                          0\n#> 3                          0\n#> 4                          0\n#> 5                          0\n#> 6                          0\n#> 7                          0\n#> 8                          0\n#> 9                          0\n#> 10                         0\n#> 11                         2\n#> 12                         4\n#> 13                         6\n#> 14                         4\n#> 15                         2\n#> 16                         0\n#> 17                         6\n#> 18                        13\n#> 19                        19\n#> 20                        13\n#> 21                         6\n#> 22                         0\n#> 23                         0\n#> 24                         0\n#> 25                         0"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#result-plot-for-the-challenge",
    "href": "content/01_journal/02_data_acquisition.html#result-plot-for-the-challenge",
    "title": "Data Acquisition",
    "section": "\n2.2 Result plot for the challenge:",
    "text": "2.2 Result plot for the challenge:\nEven though the program can print a lot of data, for sample I am plotting 15 values from the table for reference in case if online running do not work.\n\n\n\n\n\n\n\n\n\nDate and Time\nTemperature\nRelative Humidity\nRain\nPrecipitation Probability\n\n\n\n2023-05-20T00:00\n10.3\n79\n0\n0\n\n\n2023-05-20T01:00\n10.5\n82\n0\n0\n\n\n2023-05-20T02:00\n9.8\n87\n0\n0\n\n\n2023-05-20T03:00\n9.7\n87\n0\n0\n\n\n2023-05-20T04:00\n9.6\n89\n0\n0\n\n\n2023-05-20T05:00\n10.3\n87\n0\n0\n\n\n2023-05-20T06:00\n11.2\n85\n0\n0\n\n\n2023-05-20T07:00\n12.8\n80\n0\n0\n\n\n2023-05-20T08:00\n14.3\n72\n0\n0\n\n\n2023-05-20T09:00\n15.4\n68\n0\n0\n\n\n2023-05-20T10:00\n16.6\n64\n0\n2\n\n\n2023-05-20T11:00\n18.5\n56\n0\n4\n\n\n2023-05-20T12:00\n19.7\n50\n0\n6\n\n\n2023-05-20T13:00\n20.6\n47\n0\n4\n\n\n2023-05-20T14:00\n20.0\n50\n0\n2"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#steps-involved-in-solution",
    "href": "content/01_journal/02_data_acquisition.html#steps-involved-in-solution",
    "title": "Data Acquisition",
    "section": "\n3.1 Steps involved in Solution:",
    "text": "3.1 Steps involved in Solution:\n\nLoad the necessary libraries: rvest and dplyr.\nDefine the URL of the competitor website.\nRead the HTML content of the website using read_html().\nScrape the model names by selecting the appropriate HTML elements using CSS selectors (html_nodes()), extracting the text (html_text()), and trimming white spaces (trimws()).\nScrape the prices by following a similar process as step 4, but also perform additional data cleaning steps such as removing non-numeric characters, replacing commas with dots, and converting the values to numeric.\nDefine a reasonable price range (maximum price).\nFilter out rows with prices exceeding the reasonable price range.\nCreate a new variable indicating whether the price is reasonable or not.\nIterate over the prices and reasonability using a loop, printing the price and its reasonability.\nCreate a data frame (data) with the scraped model names, prices, and reasonability.\nGenerate a bar plot of the prices by model and reasonability, where reasonable prices are shown in green and unreasonable prices are shown in red.\nPrint the first 10 rows of the data frame.\n\nThe complete code:\n\n##############################################################################################\n#2nd task\n##############################################################################################\nlibrary(rvest)\nlibrary(dplyr)\n\n# Define the URL of the competitor website\nurl <- \"https://www.radon-bikes.de/en/mountainbike/hardtail/bikegrid/\"\n\n# Read the HTML content of the website\nhtml <- read_html(url)\n\n# Scrape the model names and prices\nmodel_names <- html %>%\n  html_nodes(\".m-bikegrid__info .a-heading--small\") %>%\n  html_text()%>%\n  trimws()\n\nprices <- html %>%\n  html_nodes(\".m-bikegrid__price--active\") %>%\n  html_text() %>%\n  trimws() %>%\n  gsub(\"[^0-9.,]\", \"\", .) %>%\n  gsub(\",\", \".\", .) %>%\n  as.numeric()\n  \nprices <- na.omit(prices)\n\n# Define the reasonable price range\nmax_price <- 2000  # Maximum reasonable price\n\n# Filter out rows with unreasonable prices\nreasonable_prices <- prices <= max_price\n\nreasonable_list <- ifelse(reasonable_prices, \"Reasonable\", \"Not Reasonable\")\n\n# uncomment this block if you want to print the reasonability individually\n#for (i in seq_along(prices)) {\n#  price <- prices[i]\n#  reasonable <- ifelse(reasonable_prices[i], \"Reasonable\", \"Not Reasonable\")\n#  print(paste(\"Price:\", price, \"| Reasonable:\", reasonable))\n#}\n\n# # Create a data frame with the scraped data\ndata <- data.frame(Model = model_names, Price = prices, Reasonability = reasonable_list)\n\nbarplot(as.numeric(data$Price), names.arg = data$Model, col = ifelse(data$Reasonability == \"Reasonable\", \"green\", \"red\"),\n        xlab = \"Model\", ylab = \"Price\", main = \"Price by Model and Reasonability\")\n\n\n\n\n\n\n# Print the first 10 rows of the data frame\nprint(head(data, n = 10))\n\n#>                Model   Price  Reasonability\n#> 1        JEALOUS 8.0 2520.17 Not Reasonable\n#> 2        JEALOUS 9.0 2856.30 Not Reasonable\n#> 3       JEALOUS 10.0 4200.84 Not Reasonable\n#> 4    JEALOUS 10.0 EA 5461.34 Not Reasonable\n#> 5    JEALOUS 10.0 EA 3780.67 Not Reasonable\n#> 6     JEALOUS AL 8.0 1007.56     Reasonable\n#> 7  JEALOUS AL 8.0 HD 1427.73     Reasonable\n#> 8     JEALOUS AL 9.0 1511.76     Reasonable\n#> 9     JEALOUS AL 9.0 1511.76     Reasonable\n#> 10   JEALOUS AL 10.0 1679.83     Reasonable"
  },
  {
    "objectID": "content/01_journal/02_data_acquisition.html#result-plot",
    "href": "content/01_journal/02_data_acquisition.html#result-plot",
    "title": "Data Acquisition",
    "section": "\n3.2 Result Plot:",
    "text": "3.2 Result Plot:\nThe result that was obtained from the local machine. The color of the reasonability has been changed from green, red to blue and yellow for the diffrerentiation:\n\n\nPrice by Model and Reasonability"
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html",
    "href": "content/01_journal/03_data_wrangling.html",
    "title": "Data Wrangling",
    "section": "",
    "text": "Data wrangling in R refers to the process of transforming and manipulating data to make it more suitable for analysis or further processing. It involves tasks such as cleaning, reshaping, filtering, aggregating, and merging datasets. Data wrangling is an essential step in data analysis and is often performed using packages like dplyr and tidyr within the tidyverse ecosystem.\nHere are some key concepts and techniques commonly used in data wrangling in R:\n\nData Import: Reading data from various file formats such as CSV, Excel, or databases into R using functions like read_csv(), read_excel(), or read.table().\nData Cleaning: Handling missing values (na.omit(), complete.cases()), removing duplicates (distinct()), dealing with outliers, and correcting data inconsistencies.\nData Transformation: Modifying data structure and content using functions like mutate(), select(), filter(), and arrange() from the dplyr package. Renaming variables, creating new variables, or recoding values are common transformation tasks.\nReshaping Data: Converting data between wide and long formats using functions like pivot_longer() and pivot_wider() from the tidyr package. This is useful when dealing with data that needs to be organized differently for analysis or visualization.\nAggregating Data: Summarizing data by groups using functions like group_by() and summarize(). Calculating group-level statistics, such as means, counts, or proportions, is common in data analysis.\nMerging and Joining Data: Combining multiple datasets based on common variables using functions like merge() or left_join(). This allows you to bring together different pieces of information from different sources.\nHandling Dates and Times: Manipulating and extracting information from date and time variables using functions from packages like lubridate.\nData Type Conversion: Converting data types to the appropriate format, such as converting character variables to factors or numeric variables to dates.\n\nOverall, data wrangling in R involves a combination of functions and techniques to transform, clean, and reshape data in preparation for analysis. It requires a good understanding of the dataset and the available tools in R to effectively manage and manipulate data."
  },
  {
    "objectID": "content/01_journal/03_data_wrangling.html#solution",
    "href": "content/01_journal/03_data_wrangling.html#solution",
    "title": "Data Wrangling",
    "section": "\n2.1 Solution:",
    "text": "2.1 Solution:\nIn this challenge I have used reduced dataset for the year 2014.\n\nPatent Dominance: What US company / corporation has the most patents? List the 10 US companies with the most assigned/granted patents.\n\n\n##############################################################################################\n#1st task\n##############################################################################################\n\n#using data.table\nlibrary(data.table)\n\n# Step 1: Read the asignee.tsv file\nasignee_dt <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/assignee.tsv\")\n\n# Step 2: Read the patent_asignee.tsv file\npatent_asignee_dt <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/patent_assignee.tsv\")\n\n# Merge the data.tables based on assignee_id\nmerged_dt <- merge(asignee_dt, patent_asignee_dt, by.x = \"id\", by.y = \"assignee_id\")\n\n# Count the number of patents for each organization\npatent_counts <- merged_dt[, .N, by = organization]\n\n# Sort the patent counts in descending order\nsorted_counts <- patent_counts[order(-N)]\n\n# Select the top 10 companies with the most patents\ntop_10_companies <- sorted_counts[1:10]\n\n# Print the result\nprint(top_10_companies)\n\n#>                                    organization    N\n#>  1: International Business Machines Corporation 7547\n#>  2:               Samsung Electronics Co., Ltd. 5835\n#>  3:                      Canon Kabushiki Kaisha 4099\n#>  4:                            Sony Corporation 3326\n#>  5:                       Microsoft Corporation 3165\n#>  6:                                 Google Inc. 2668\n#>  7:                    Kabushiki Kaisha Toshiba 2656\n#>  8:                       QUALCOMM Incorporated 2597\n#>  9:                         LG Electronics Inc. 2459\n#> 10:                                        <NA> 2377\n\n\n\nRecent patent activity: What US company had the most patents granted in August 2014? List the top 10 companies with the most new granted patents for August 2014.\n\n\n##############################################################################################\n# 2nd task:\n##############################################################################################\n\nlibrary(data.table)\n\n# Read the files into data.tables\nasignee_dt <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/assignee.tsv\")\npatent_asignee_dt <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/patent_assignee.tsv\")\npatent_dt <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/patent.tsv\")\n\n# Merge the data.tables based on assignee_id\nmerged_dt <- merge(merge(asignee_dt, patent_asignee_dt, by.x = \"id\", by.y = \"assignee_id\"), patent_dt, by.x = \"patent_id\", by.y = \"id\")\n\n# Filter the merged data.table for patents granted in August 2014\naugust_2014_patents <- merged_dt[lubridate::year(date) == 2014 & lubridate::month(date) == 8]\n\n# Count the number of patents for each organization in August 2014\npatent_counts <- august_2014_patents[, .N, by = organization]\n\n# Order the counts in descending order\nordered_counts <- patent_counts[order(-N)]\n\n# Select the top 10 companies with the most new granted patents in August 2014\ntop_10_companies_in_august <- ordered_counts[1:10]\n\n# Print the result\nprint(top_10_companies_in_august)\n\n#>                                    organization   N\n#>  1: International Business Machines Corporation 718\n#>  2:               Samsung Electronics Co., Ltd. 524\n#>  3:                      Canon Kabushiki Kaisha 361\n#>  4:                       Microsoft Corporation 337\n#>  5:                            Sony Corporation 269\n#>  6:                                 Google Inc. 240\n#>  7:                       QUALCOMM Incorporated 223\n#>  8:                                  Apple Inc. 222\n#>  9:                    Kabushiki Kaisha Toshiba 213\n#> 10:                         LG Electronics Inc. 211\n\n\n\nInnovation in Tech: What is the most innovative tech sector? For the top 10 companies (worldwide) with the most patents, what are the top 5 USPTO tech main classes?\n\n\n##############################################################################################\n#3rd task\n##############################################################################################\n\nlibrary(data.table)\n\n# Read the files into data tables\nasignee <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/assignee.tsv\")\npatent_asignee <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/patent_assignee.tsv\")\nuspc <- fread(\"C:/Users/LENOVO/OneDrive/Desktop/business_module/ss23-bdsb-SubramanyaGurumurthy/Patent_data_reduced/uspc.tsv\")\n\n# Merge the data tables\nmerged_dt <- merge(asignee, patent_asignee, by.x = \"id\", by.y = \"assignee_id\")\nmerged_dt <- merge(merged_dt, uspc, by = \"patent_id\")\n\n# Count the number of patents per company\npatents_per_company <- merged_dt[, .(patent_count = .N), by = organization]\ntop_10_companies <- patents_per_company[order(-patent_count)][1:10]\n\n# Get the top 5 USPTO tech main classes for the top 10 companies\ntop_10_company_ids <- top_10_companies$organization\ntop_5_main_classes <- merged_dt[organization %in% top_10_company_ids, .N, by = mainclass_id][order(-N)][1:5]\n\n# Print the results\nmost_innovative_tech_sector <- top_5_main_classes$mainclass_id\nprint(paste(\"Most innovative tech sectors are: \", most_innovative_tech_sector))\n\n#> [1] \"Most innovative tech sectors are:  257\"\n#> [2] \"Most innovative tech sectors are:  455\"\n#> [3] \"Most innovative tech sectors are:  370\"\n#> [4] \"Most innovative tech sectors are:  348\"\n#> [5] \"Most innovative tech sectors are:  709\""
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html",
    "href": "content/01_journal/04_data_visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Data visualization in R involves creating visual representations of data to explore patterns, relationships, and trends. R provides several packages and libraries that offer a wide range of tools for data visualization, including base R graphics, ggplot2, lattice, and plotly.\nHere are some key points about data visualization in R:"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html#challenge-1",
    "href": "content/01_journal/04_data_visualization.html#challenge-1",
    "title": "Data Visualization",
    "section": "\n1.1 Challenge 1:",
    "text": "1.1 Challenge 1:\nGoal: Map the time course of the cumulative Covid-19 cases! Your plot should look like this:\n\n\nChallenge 1\n\n\nAdding the cases for Europe is optional. You can choose your own color theme, but don’t use the default one. Don’t forget to scale the axis properly. The labels can be added with geom_label() or with geom_label_repel() (from the package ggrepel).\nSolution:\n\n##############################################################################################\n#1st task\n##############################################################################################\n\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(lubridate)\n\n# Read the data from the CSV file\ndata <- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#> Rows: 311391 Columns: 67\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Filter data for the selected countries in Europe and the United States\ncountries <- c(\"Germany\", \"United Kingdom\", \"France\", \"Spain\", \"United States\")\nfiltered_data <- data %>%\n  dplyr::filter(location %in% countries)\n\nfiletered_conti <- data %>%\n  dplyr::filter(location %in% c(\"Europe\"))\n\n# Convert date column to date format\nfiltered_data$date <- ymd(filtered_data$date)\nfiletered_conti$date <- ymd(filetered_conti$date)\n\n# Calculate the cumulative cases\nfiltered_data <- filtered_data %>%\n  group_by(date, location) %>%\n  summarize(cumulative_cases = sum(total_cases, na.rm = TRUE)) %>%\n  ungroup()\n\n#> `summarise()` has grouped output by 'date'. You can override using the\n#> `.groups` argument.\n\nfiletered_conti <- filetered_conti %>%\n  group_by(date) %>%\n  summarize(cumulative_cases = sum(total_cases, na.rm = TRUE),\n            location = \"Europe\") %>%\n  ungroup()\n\nfiltered_data <- bind_rows(filtered_data, filetered_conti)\n\n\n# Convert total cases to millions\nfiltered_data$cumulative_cases <- filtered_data$cumulative_cases / 1e6\n\n# Filter data until May 2022\nfiltered_data <- filtered_data %>%\n  dplyr::filter(date <= ymd(\"2022-05-01\"))\n\n# Find the highest values of cases for Europe and the United States\nhighest_cases <- filtered_data %>%\n  dplyr::filter(location %in% c(\"Europe\", \"United States\")) %>%\n  group_by(location) %>%\n  dplyr::filter(cumulative_cases == max(cumulative_cases)) %>%\n  ungroup()\n\n# Plot the cumulative cases\nggplot(filtered_data, aes(x = date, y = cumulative_cases, color = location)) +\n  geom_line() +\n  geom_text(data = highest_cases, aes(label = round(cumulative_cases, 2), x = date, y = cumulative_cases),\n            color = \"black\", vjust = -0.5, size = 3) +\n  labs(x = \"Timeline in Months\", y = \"Cumulative Cases (Millions)\", title = \"Covid-19 confirmed cases worldwide\",\n       subtitle = \"Until 06-2022\") +\n  scale_x_date(date_labels = \"%b %Y\", date_breaks = \"1 month\") +\n  scale_y_continuous(labels = function(x) paste0(x, \"M\")) +\n  scale_color_manual(values = c(\"Germany\" = \"blue\", \"United Kingdom\" = \"red\",\n                                \"France\" = \"green\", \"Spain\" = \"orange\",\n                                \"United States\" = \"purple\", \"Europe\" = \"cyan\")) +\n  theme_dark() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "content/01_journal/04_data_visualization.html#challenge-2",
    "href": "content/01_journal/04_data_visualization.html#challenge-2",
    "title": "Data Visualization",
    "section": "\n1.2 Challenge 2:",
    "text": "1.2 Challenge 2:\nGoal: Visualize the distribution of the mortality rate (deaths / population) with geom_map() (alternatively, you can plot the case-fatality rate (deaths / cases)). The necessary longitudinal and lateral data can be accessed with this function:\nworld <- map_data(\"world\")\nThis data has also to be put in the map argument of geom_map():\nplot_data %>% ggplot( ... ) +\n  geom_map(aes(map_id = ..., ... ), map = world, ... ) +\n  ...\n\n\n\nChallenge 1\n\n\nSolution:\n\n##############################################################################################\n#2nd task\n##############################################################################################\nlibrary(tidyverse)\nlibrary(scales)\n\n#> \n#> Attaching package: 'scales'\n\n\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n\n\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\n\n# Read the data from the CSV file\ndata <- read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\n#> Rows: 311391 Columns: 67\n\n\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr   (4): iso_code, continent, location, tests_units\n#> dbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\n#> date  (1): date\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Convert date column to date format\ndata$date <- as.Date(data$date)\n\n# Filter data for the latest date\nlatest_data <- data %>%\n  filter(date == max(date))\n\n# Calculate mortality rate\nlatest_data <- latest_data %>%\n  mutate(mortality_rate = total_deaths / population) %>%\n  select(location, mortality_rate) %>%\n  mutate(location = case_when(\n    \n    location == \"United Kingdom\" ~ \"UK\",\n    location == \"United States\" ~ \"USA\",\n    location == \"Democratic Republic of Congo\" ~ \"Democratic Republic of the Congo\",\n    TRUE ~ location\n    \n  )) %>%\n  distinct()\n\nworld <- map_data(\"world\")\n# Perform the left join\nworld <- left_join(world, latest_data, by = c(\"region\" = \"location\"))\n\nggplot(world) +\n  geom_map(aes(map_id = region, fill= mortality_rate), map = world) +\n  expand_limits(x = world$long, y = world$lat) +\n  coord_map()+\n  scale_fill_continuous(labels = percent_format(), name = \"Mortality Rate\")+\n  labs(title=\"Confirmed Covid-19 deaths relative to the size of the Population\",\n       subtitle = \"Around 6.2 Million confirmed COVID-19 deaths worldwide\")\n\n\n\n\n\n\n\nResult from the local computer:\nI could not find the reason why the world map produced while rendering qmd page is grayscale. But in the local computer, I was able to get the actual plot in color, which can be seen here:\n\n\nConfirmed Covid deaths Related to Population in World Map"
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "Hi this is Subramanya Nanjangud Gurumurthy. I study M.Sc.Mechatronics in TU-Hamburg. My Matriculation Number is 54843. Currently I am pursuing my Master Thesis at Neura Robotics GmbH, Reiderich. I am part of the course Business Data Science Basics, as a part of business module."
  }
]